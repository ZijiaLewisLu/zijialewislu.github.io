<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="google-site-verification" content="Qdhsa2MSQ4tNMmzxdJ2CE9XcmudTKdf7ftxaZ87hNrU" />
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About Me - Zijia Lu </title>
    <meta name="description" content="Zijia Lu - PhD Candidate in Northeastern University, Research Intern at Microsoft, Amazon and NVIDIA.">
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Left Sidebar -->
        <div class="sidebar">
            <div class="profile">
                <!-- <img src="./images/photo.jpg" alt="Zijia Lu" class="profile-image"> -->
                <img src="./images/cartoon.png" alt="Zijia Lu" class="profile-image">
                <div class="profile-info">
                    <h1 class="profile-name">Zijia Lu</h1>
                    <p class="profile-title">Ph.D. Candidate in Computer Science</p>
                    <div class="contact-item">
                        <a href="mailto:lu.zij@northeastern.edu">
                            <img src="symbols/email.png" alt="Email Icon" class="contact-icon">
                            <!-- <span class="contact-text">lu.zij@northeastern.edu</span> -->
                        </a>
                        <a href="https://scholar.google.com/citations?user=xEGL7NsAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener">
                            <img src="symbols/icons8-google-scholar-50.png" alt="Google Scholar Icon" class="contact-icon">
                            <!-- <span class="contact-text">Google Scholar</span> -->
                        </a>
                        <a href="https://github.com/ZijiaLewisLu" target="_blank" rel="noopener">
                            <img src="symbols/icons8-github-30.png" alt="Github Icon" class="contact-icon">
                            <!-- <span class="contact-text">Github</span> -->
                        </a>
                        <a href="https://www.linkedin.com/in/zijialewislu" target="_blank" rel="noopener">
                            <img src="symbols/icons8-linkedin-circled-50.png" alt="LinkedIn Icon" class="contact-icon">
                            <!-- <span class="contact-text">LinkedIn</span> -->
                        </a>
                    </div>
                </div>
            </div>
        
            
            <nav class="sidebar-nav">
                <ul>
                    <li><a href="#about" class="active">About Me</a></li>
                    <li><a href="#updates">Recent Updates</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#experience">Experience</a></li>
                </ul>
            </nav>
        </div>
        
        <!-- Main Content -->
        <div class="main-content">
            <!-- About Section -->
            <div class="section" id="about">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <p>
                    I am a final-year Ph.D. Candidate in Computer Science at <org>Northeastern University</org>. 
                    My research focuses on <strong>Video Understanding Methods</strong> to address key challenges in <strong>Video-Language Models (VLM)</strong> and <strong>Assistant AI</strong>. 
                    Specifically, it includes data-efficient video-text matching, enhanced computational efficiency, accurate long temporal modeling, and procedural video understanding. 
                    Currently, I am working on <i>open-world video understanding</i> and <i>video data generation</i>.
                    </p>
                    <p>During my Ph.D. study, I have had the opportunity to spend two memorable internships at <org>Amazon AWS AI Lab</org> and <org>Microsoft Research</org>.
                    In the past, I also worked as a Student Researcher at <org>Chinese Academic of Sciences</org>, and an Architecture Summer Intern at <org>NVIDIA</org>.
                    In 2019, I received dual B.S. degrees in Computer Science and Economics from <org>NYU Shanghai</org>, and awarded with the NYU University Honors Scholar and the Undergraduate Scholarship of University of Chinese Academy of Sciences.
                    </p>
                    <p><i>I am actively seeking full-time research positions. Please reach out to me if you have an opportunity!</i></p>
                </div>
            </div>
            
            <!-- Updates Section -->
            <div class="section" id="updates">
                <h2 class="section-title">Recent Updates</h2>
                <ul class="updates-list">
                    <li class="update-item">
                        <span class="update-date">Apr 2025</span>
                        <span class="update-content">My intern paper at Microsoft is accepted to CVPR 2025.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Jun 2024</span>
                        <span class="update-content">Started Research Internship at Microsoft Responsible and Open AI Research Team.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Apr 2024</span>
                        <span class="update-content">Three papers accepted to CVPR 2024. My intern paper at Amazon is designated as "Highlight" paper.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Sep 2022</span>
                        <span class="update-content">Started as an Applied Scientist Intern at Amazon's AWS AI Lab.</span>
                    </li>
                    <li class="update-item">
                        <span class="update-date">Apr 2022</span>
                        <span class="update-content">One paper accepted to CVPR 2022.</span>
                    </li>
                </ul>
            </div>
            
            <!-- Combined Publications Section -->
            <div class="section" id="publications">
                <h2 class="section-title">Selected Publications</h2>
                <div class="research-container">

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/decaf2.png" alt="DeCafNet">
                                </div>
                                <h3 class="publication-title">DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen</p>
                                
                                <!-- Modified to display venue and links on the same line -->
                                <div>
                                    <!-- <p class="publication-venue">CVPR 2025</p> -->
                                    <div class="publication-links">
                                        <p class="publication-venue">CVPR 2025</p>
                                        <a href="https://roar-ai.github.io/decafnet/index.html">Project</a>
                                        <a href="https://arxiv.org/abs/2505.16376">Paper</a>
                                        <a href="https://github.com/ZijiaLewisLu/CVPR2025-DeCafNet">Code</a>
                                    </div>
                                </div>
                                
                            </div>
                            <div class="research-item-right">
                                <!-- <ul class="publication-description" style="margin:0; padding-left:5px;"> -->
                                <ul class="publication-description">
                                    <li class="regular">A Delegate-and-Conquer framework for efficient coarse-to-fine <span>Long Video Temporal Grounding</span>. </li>
                                    <li class="regular"><span>Efficient Video Encoder</span> for end-to-end training on Ego4D with one A100. </li>
                                    <li class="regular">SOTA accuracy with 47%-66% lower computation.</li>
                                    <li class="star">APPLICATION: improve <span>VLLMs</span> to efficiently handle hour-long videos.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/fact.png" alt="FACT">
                                </div>
                                <h3 class="publication-title">FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Fully-Supervised Action Segmentation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                
                                <!-- Updated layout to match first research item -->
                                <div>
                                    <!-- <p class="publication-venue">CVPR 2024</p> -->
                                    <div class="publication-links">
                                        <p class="publication-venue">CVPR 2024</p>
                                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper.pdf">Paper</a>
                                        <a href="https://github.com/ZijiaLewisLu/CVPR2024-FACT">Code</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <!-- Converted paragraph to bulleted list -->
                                <ul class="publication-description">
                                    <li class="regular">New <span>Long Temporal Reasoning</span> paradigm with parallel modeling of frame details and temporal events/actions.</li>
                                    <li class="regular">Action Tokens Design for dynamic video condensing and enabling text-data input.</li>
                                    <!-- <li class="regular">Better feature semantic with one-to-one/many token.</li> -->
                                    <!-- <li class="regular">Efficient framework of synchronized temporal modeling on multi-levels (frame/action).</li> -->
                                    <li class="regular">SOTA accuracy with 3x inference speed.</li>
                                    <!-- <li class="regular">Enabled Vision-Language Learning for action understanding.</li> -->
                                    <li class="star">APPLICATION: better <span>Assistant AI</span>; allow <span>Multi-Modality Learning</span>. </li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/egoper.png" alt="Error Detection">
                                </div>
                                <h3 class="publication-title">Error Detection in Egocentric Procedural Task Videos</h3>
                                <p class="publication-authors">Shih-Po Lee, <strong>Zijia Lu</strong>, Zekun Zhang, Minh Hoai, Ehsan Elhamifar</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">CVPR 2024</p>
                                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf">Paper</a>
                                        <a href="https://github.com/robert80203/EgoPER_official">Code & Dataset</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <li class="regular">A <span>One-class Error Detection</span> method in procedural videos.</li>
                                    <!-- <li class="regular">Spatio-Temporal and Contrastive Learning method for error detection in procedural videos.</li> -->
                                    <li class="regular">First Egocentric Procedural Error Detection (EgoPER) dataset with extensive error types.</li>
                                    <li class="star">APPLICATION: error detection for <span>Egocentric Understanding</span> and <span>Assistant AI</span>.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/pcl.png" alt="Self-Supervised Multi-Object Tracking">
                                </div>
                                <h3 class="publication-title">Self-Supervised Multi-Object Tracking with Path Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">CVPR 2024 Highlight</p>
                                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Self-Supervised_Multi-Object_Tracking_with_Path_Consistency_CVPR_2024_paper.pdf">Paper</a>
                                        <a href="https://github.com/amazon-science/path-consistency">Code</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <li class="regular">Self-Supervised Consistency Loss for robust <span>Multi-Object Tracking</span> without manual labels.</li> 
                                    <li class="regular">Superior or comparable to supervised methods on popular benchmarks.</li>
                                    <li class="star">APPLICATION: Robust object tracking for <span>Egocentric Understanding</span> and <span>Scene Understanding</span>.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/poc.png" alt="Set-Supervised Action Learning">
                                </div>
                                <h3 class="publication-title">Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">CVPR 2022</p>
                                        <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/setSupSegmentation-CVPR22.pdf">Paper</a>
                                        <a href="https://github.com/ZijiaLewisLu/CVPR22-POC">Code</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <li class="regular"><span>Video-Text Alignment</span> between video frames and unordered sets of actions in video parsed from video narrations.</li>
                                    <li class="regular">New differentiable <span>Sequence Metric</span> for weakly-supervised video-text alignment.</li>
                                    <!-- <li class="regular">Doubled the accuracies of state-of-the-art approaches for weakly supervised video-text learning.</li> -->
                                    <!-- <li class="star">APPLICATION: New Action Modeling Approach. learn shared video-text semantic space without human label.</li> -->
                                    <li class="star">APPLICATION: learn video-text semantic space for <span>VLLM</span>or <span>Video Generation</span>.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/tasl.png" alt="Weakly-supervised Action Segmentation">
                                </div>
                                <h3 class="publication-title">Weakly-supervised Action Segmentation and Alignment via Transcript-Aware Union-of-subspaces Learning</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Ehsan Elhamifar</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">ICCV 2022</p>
                                        <a href="http://www.khoury.northeastern.edu/home/eelhami/publications/TASL-ICCV21.pdf">Paper</a>
                                        <a href="https://github.com/ZijiaLewisLu/ICCV21-TASL">Code</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <!-- <li class="regular">Action Modeling via Multi-Dimension Subspaces to capture large intra-class variations in weakly supervised video-text learning scenarios.</li> -->
                                    <li class="regular">New Union-of-Subspace network for accurate <span>Action Modeling</span> and capturing complex action variations.</li>
                                    <li class="regular"><span>Contrastive Learning</span> for weakly-supervised video-text alignment.</li>
                                    <!-- <li class="regular">Union-of-Subspace Network for better Action Modeling via Multi-Dimension Subspaces to capture large intra-class variations in weakly supervised video-text learning scenarios.</li> -->
                                    <li class="star">APPLICATION: <span>Video-Text Alignment</span>.</li>
                                    <!-- <li class="regular">Proposed Self-Supervised method to Recover and Learn Action Temporal Dependencies.</li>
                                    <li class="regular">Doubled the accuracies of state-of-the-art approaches for weakly supervised video-text learning.</li>
                                    <li class="star">APPLICATION: New Action Modeling Approach. learn shared video-text semantic space without human label.</li> -->
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/dft-net.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Dft-Net: Disentanglement of Face Deformation and Texture Synthesis for Expression Editing</h3>
                                <p class="publication-authors">Jinghui Wang, Jie Zhang, <strong>Zijia Lu</strong>, Shiguang Shan</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">ICIP 2019</p>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/8803416">Paper</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <li class="regular">Two-Branch <span>GAN</span> network for facial expression edition.</li>
                                    <li class="regular">Warping branch for expression transform and Generative branch for refinement.</li>
                                    <li class="star">APPLICATION: Controlled <span>Image Generation</span> and <span>Expression Editing</span>.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="research-item">
                        <div class="research-item-top">
                            <div class="research-item-left">
                                <div class="research-item-image">
                                    <img src="./images/zml2p.png" alt="Zero-Shot Facial Expression Recognition">
                                </div>
                                <h3 class="publication-title">Zero-Shot Facial Expression Recognition with Multi-Label Label Propagation</h3>
                                <p class="publication-authors"><strong>Zijia Lu</strong>, Jiabei Zeng, Shiguang Shan, Xilin Chen</p>
                                
                                <div>
                                    <div class="publication-links">
                                        <p class="publication-venue">ACCV 2018 Oral</p>
                                        <a href="https://link.springer.com/chapter/10.1007/978-3-030-20893-6_2">Paper</a>
                                        <a href="https://github.com/ZijiaLewisLu/ACCV18-ZML2P">Code & Dataset</a>
                                    </div>
                                </div>
                            </div>
                            <div class="research-item-right">
                                <ul class="publication-description">
                                    <li class="regular">Transductive Label Propagation method for <span>Zero-Shot</span> facial expression recognition.</li>
                                    <li class="regular">The first Open-Set Facial Expression Recognition dataset.</li>
                                    <li class="star">APPLICATION: <span>Open-World</span> affection computing for human-computer interaction.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    
                </div>
            </div>

            <!-- Experience Section -->
            <div class="section" id="experience">
                <h2 class="section-title">Experience</h2>
                <div class="experience-container">
                    <div class="experience-item">
                        <img src="./symbols/Microsoft.jpg" alt="Microsoft" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Research Intern at Microsoft</h3>
                                <span class="experience-duration" data-nosnippet="true">06/2024 - 09/2024</span>
                            </div>
                            <p class="experience-description">Worked on efficient end-to-end models for text-based video temporal grounding.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/amazon-logo-on-transparent-background-free-vector.jpg"" alt="Amazon" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Applied Scientist Intern at Amazon AWS AI Lab</h3>
                                <!-- Wrap in a data-nosnippet attribute to help prevent auto-formatting -->
                                <span class="experience-duration" data-nosnippet="true">09/2022 - 03/2023</span>
                            </div>
                            <p class="experience-description">Worked on Self-Supervised and Robust Multi-Object Tracking.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/northeastern.png" alt="Northeastern University" class="experience-image">
                        <div class="experience-info">
                            <!-- <span class="experience-duration">2019 - Present</span> -->
                            <div class="experience-meta">
                                <!-- <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3> -->
                                <h3 class="experience-title">Graduate Research Assistant in Northeastern University</h3>
                                <!-- <span class="experience-organization">Northeastern University</span> -->
                                <span class="experience-duration" data-nosnippet="true">09/2019 - Present</span>
                            </div>
                            <p class="experience-description">Focusing on Video-Text Understanding, Action Segmentation, Egocentric Understanding and Video Data Generation.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="./symbols/cas2.jpg" alt="" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Student Researcher at Chinese Academy of Science</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration" data-nosnippet="true">01/2018 - 12/2018</span>
                            </div>
                            <p class="experience-description">Worked on Zero-Shot Facial Recognition and Expression Editing.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="symbols/nvidia2.png" alt="NVIDIA" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">Architecture Summer Intern at NVIDIA Shanghai</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration" data-nosnippet="true">06/2016 - 09/2016</span>
                            </div>
                            <p class="experience-description">Developed CUDNN function for Volta GPU and GPU performance simulator.</p>
                        </div>
                    </div>
                    <div class="experience-item">
                        <img src="symbols/nyush2.jpg" alt="NYU Shanghai" class="experience-image">
                        <div class="experience-info">
                            <div class="experience-meta">
                                <h3 class="experience-title">B.S in Computer Science and Economics at NYU Shanghai</h3>
                                <!-- <span class="experience-organization">NYU Shanghai</span> -->
                                <span class="experience-duration" data-nosnippet="true">2014 - 2019 (Gap Year 2018)</span>
                            </div>
                            <p class="experience-description">Graduated with a Major GPA of 3.98/4. Conducted research in Image Segmentation with Prof. Zhen Zhang and Document QA with Prof. Kyunghyun Cho.</p>
                        </div>
                    </div>
                    
                </div>
            </div>
        </div>
    </div>
    
    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Zijia Lu</p>
    </footer>
    
    <script src="script.js"></script>
</body>
</html>